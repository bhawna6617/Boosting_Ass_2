{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a243e3",
   "metadata": {},
   "source": [
    "# quest 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629d5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vGradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict a continuous target variable. It belongs to the family of ensemble methods, which combine predictions from multiple individual models to produce a more accurate final prediction.\n",
    "\n",
    "# Here's how Gradient Boosting Regression works:\n",
    "\n",
    "# Boosting Technique: Gradient Boosting builds an ensemble of weak learners (usually decision trees) sequentially. Each new model in the sequence corrects the errors made by the previous ones, hence boosting the overall performance.\n",
    "# Gradient Descent: At each stage of training, the algorithm fits a new model to the residual errors made by the previous models. This is where the term \"gradient\" comes from; it's about minimizing the loss function (typically mean squared error for regression tasks) by descending along the gradient.\n",
    "# Trees as Base Learners: Decision trees are commonly used as base learners in Gradient Boosting. Each tree is trained to predict the residuals (the differences between the predicted and actual values) of the ensemble's current approximation.\n",
    "# Additive Modeling: The predictions of all the weak learners are combined together to produce the final prediction. This additive modeling approach means that each new model in the sequence is trained to predict the residuals of the ensemble's current approximation rather than the original target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323748bb",
   "metadata": {},
   "source": [
    "# quest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd776c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ensemble model with a constant value (usually the mean of the target variable).\n",
    "# Fit a weak learner (decision tree) to the residuals of the current ensemble model.\n",
    "# Update the ensemble model by adding a fraction of the predictions of the weak learner.\n",
    "# Repeat steps 2-3 for a predefined number of iterations (or until convergence).\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# class GradientBoostingRegressor:\n",
    "#     def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "#         self.n_estimators = n_estimators\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.max_depth = max_depth\n",
    "#         self.models = []\n",
    "#         self.f_0 = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         # Step 1: Initialize the ensemble model with the mean of the target variable\n",
    "#         self.f_0 = np.mean(y)\n",
    "\n",
    "#         # Initialize residuals\n",
    "#         residuals = y - self.f_0\n",
    "\n",
    "#         for _ in range(self.n_estimators):\n",
    "#             # Step 2: Fit a weak learner (decision tree) to the residuals\n",
    "#             tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "#             tree.fit(X, residuals)\n",
    "\n",
    "#             # Step 3: Update the ensemble model\n",
    "#             self.models.append(tree)\n",
    "#             predictions = tree.predict(X)\n",
    "#             residuals -= self.learning_rate * predictions\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         # Make predictions by summing predictions from all weak learners and adding the initial constant\n",
    "#         y_pred = np.zeros(len(X))\n",
    "#         for tree in self.models:\n",
    "#             y_pred += self.learning_rate * tree.predict(X)\n",
    "#         y_pred += self.f_0\n",
    "#         return y_pred\n",
    "\n",
    "# # Example usage:\n",
    "\n",
    "# # Generate a simple dataset\n",
    "# np.random.seed(0)\n",
    "# X = np.random.rand(100, 1) * 10\n",
    "# y = 2 * X.squeeze() + np.random.randn(100)  # True relationship y = 2x + noise\n",
    "\n",
    "# # Initialize and train the Gradient Boosting Regressor\n",
    "# gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "# gb_regressor.fit(X, y)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = gb_regressor.predict(X)\n",
    "\n",
    "# # Evaluate the model\n",
    "# mse = mean_squared_error(y, y_pred)\n",
    "# r2 = r2_score(y, y_pred)\n",
    "\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "# print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f54e87",
   "metadata": {},
   "source": [
    "# quest 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d9f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# # Define a scorer (negative mean squared error since GridSearchCV and RandomizedSearchCV maximize the score)\n",
    "# scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# # Define hyperparameters grid for grid search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 150],\n",
    "#     'learning_rate': [0.05, 0.1, 0.2],\n",
    "#     'max_depth': [2, 3, 4]\n",
    "# }\n",
    "\n",
    "# # Initialize the Gradient Boosting Regressor\n",
    "# gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# # Perform grid search\n",
    "# grid_search = GridSearchCV(gb_regressor, param_grid, scoring=scorer, cv=5)\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# print(\"Grid Search Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Grid Search Best Negative MSE:\", grid_search.best_score_)\n",
    "\n",
    "# # Perform random search\n",
    "# random_search = RandomizedSearchCV(gb_regressor, param_distributions=param_grid, n_iter=10, scoring=scorer, cv=5)\n",
    "# random_search.fit(X, y)\n",
    "\n",
    "# print(\"Random Search Best Parameters:\", random_search.best_params_)\n",
    "# print(\"Random Search Best Negative MSE:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd77ed",
   "metadata": {},
   "source": [
    "# quest 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da7d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In the context of Gradient Boosting, a weak learner refers to a base model that performs slightly better than random guessing on a given problem. These weak learners are typically simple models, such as decision trees with shallow depths or linear models.\n",
    "\n",
    "# The term \"weak learner\" is relative; it doesn't imply that the model is inherently weak in an absolute sense, but rather that it's not strong enough to solve the problem on its own. However, when combined with other weak learners in an ensemble, they collectively form a strong predictive model.\n",
    "\n",
    "# In Gradient Boosting, each weak learner is trained sequentially to correct the errors made by the previous models. Specifically, at each iteration, a new weak learner is trained to predict the residuals (the differences between the predicted and true values) of the current ensemble model. By continuously fitting weak learners to the residuals, Gradient Boosting gradually reduces the errors in predictions and improves the overall performance of the ensemble.\n",
    "\n",
    "# Common examples of weak learners used in Gradient Boosting include decision trees with limited depths (also known as decision stumps), linear regression models, and shallow neural networks. These models are computationally efficient and can capture simple patterns in the data, making them suitable for boosting algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93b6ac",
   "metadata": {},
   "source": [
    "# quest 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77286cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Improvement: Gradient Boosting builds an ensemble model by sequentially adding weak learners, where each new learner corrects the errors made by the existing ensemble. This sequential nature allows the algorithm to gradually improve the model's predictive accuracy.\n",
    "# Gradient Descent: At each iteration, Gradient Boosting fits a weak learner to the residuals (the differences between the predicted and true values) of the current ensemble model. By using gradient descent, the algorithm minimizes the loss function (typically mean squared error for regression tasks) by moving in the direction of steepest descent in the loss landscape.\n",
    "# Combining Weak Predictors: Although individual weak learners may have limited predictive power, their collective predictions, when combined appropriately, can yield a strong predictive model. Gradient Boosting achieves this by assigning weights to each weak learner based on their contribution to minimizing the loss function.\n",
    "# Regularization: Gradient Boosting includes regularization techniques, such as shrinkage (learning rate) and tree constraints (maximum depth), to prevent overfitting and improve generalization performance. These regularization methods help control the complexity of the ensemble model and reduce the risk of memorizing the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4837c",
   "metadata": {},
   "source": [
    "# quest 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e668591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradient Boosting builds an ensemble of weak learners in a sequential manner, where each new learner is trained to correct the errors made by the existing ensemble. Here's a step-by-step explanation of how the Gradient Boosting algorithm constructs this ensemble:\n",
    "\n",
    "# Initialize the Ensemble: Gradient Boosting starts by initializing the ensemble with a simple model, often by setting initial predictions to be the mean (or some other reasonable value) of the target variable. This initial model serves as the base prediction upon which subsequent models will improve.\n",
    "# Calculate Residuals: After making initial predictions, the algorithm calculates the residuals, which represent the differences between the predicted and actual target values. These residuals indicate where the initial model's predictions are incorrect.\n",
    "# Fit a Weak Learner to Residuals: The next step is to fit a weak learner (commonly a decision tree) to the residuals of the current ensemble. The weak learner's task is to capture the patterns or relationships in the data that the current ensemble has failed to capture adequately.\n",
    "# Update Ensemble with New Learner: Once the weak learner is trained, its predictions are combined with the predictions of the existing ensemble. However, rather than adding the weak learner's predictions directly to the ensemble, Gradient Boosting employs a \"learning rate\" parameter to control the contribution of each weak learner. The learning rate scales the predictions of the weak learner before they are added to the ensemble, effectively controlling the step size in the optimization process.\n",
    "# Iterative Refinement: Steps 2-4 are repeated for a predefined number of iterations (or until a stopping criterion is met). In each iteration, a new weak learner is trained to correct the errors of the current ensemble, and its predictions are combined with those of the ensemble with appropriate weighting.\n",
    "# Final Prediction: Once all weak learners are trained and added to the ensemble, the final prediction is obtained by aggregating the predictions of all weak learners, typically through summation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df166787",
   "metadata": {},
   "source": [
    "# quest 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding several key concepts and steps. Here's a high-level overview of the main steps involved:\n",
    "\n",
    "# Loss Function: The first step is to define a loss function that measures the difference between the model's predictions and the actual target values. Common loss functions for regression tasks include mean squared error (MSE) and absolute error (L1 loss). For classification tasks, cross-entropy loss is often used.\n",
    "# Gradient Descent: Gradient Boosting minimizes the loss function using gradient descent. The gradient of the loss function with respect to the model's predictions provides information about the direction and magnitude of the error. By descending along the gradient, the algorithm updates the model's parameters to minimize the loss.\n",
    "# Base Model: Gradient Boosting starts with an initial base model, which typically predicts the mean (or some other reasonable value) of the target variable. This base model serves as the starting point for subsequent iterations.\n",
    "# Residual Calculation: After making initial predictions, Gradient Boosting calculates the residuals, which represent the errors between the predicted and actual target values. These residuals indicate where the current model's predictions are incorrect.\n",
    "# Fitting Weak Learners: In each iteration, Gradient Boosting fits a weak learner (e.g., decision trees) to the residuals of the current model. The weak learner's task is to capture the patterns or relationships in the data that the current model has failed to capture adequately.\n",
    "# Learning Rate: Gradient Boosting employs a \"learning rate\" parameter to control the contribution of each weak learner. The learning rate scales the predictions of the weak learner before they are added to the ensemble, effectively controlling the step size in the optimization process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
